{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a previously created model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sagemaker decouples model creation/fitting and model deployment. **This short notebook shows how you can deploy a model that you have already created**. It is assumed that you have already created the model and it appears in the `Models` section of the SageMaker console. Obviously, before you deploy a model the model must exist, so please go back and make sure you have already fit/created the model before proceeding. \n",
    "For more information about deploying models, see https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-deploy-model.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import gmtime,strftime\n",
    "import datetime\n",
    "import sagemaker\n",
    "\n",
    "sm = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "training_image = retrieve(framework='xgboost', region='us-east-1', version='latest')\n",
    "training_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "%store -r bucket prefix TrainingJobName\n",
    "model_path = f\"s3://{bucket}/{prefix}/output/{TrainingJobName}/output\"\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-15 13:46:22       4579 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Using the following command you can verify that the model tar file exists in the model_path\n",
    "!aws s3 ls $model_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bucket,  model_name, model_path, container, role):\n",
    "    try:\n",
    "        sm.create_model(\n",
    "            ModelName=model_name,\n",
    "            PrimaryContainer={\n",
    "                'Image': container,\n",
    "                'ModelDataUrl': f'{model_path}/model.tar.gz',\n",
    "                'Environment': {\n",
    "                    'SAGEMAKER_REGION': 'us-east-1'\n",
    "                }\n",
    "            },\n",
    "            ExecutionRoleArn=role\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f'Error while creating model {model_name}: {e}')\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model-'+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())   # ^[a-zA-Z0-9](-*[a-zA-Z0-9])*\n",
    "create_model(bucket, model_name, model_path, training_image, role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy using an inference endpoint (Optional)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#set endpoint name/config.\n",
    "endpoint_config_name = 'DEMO-model-config-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_name = 'DEMO-model-config-'  + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n",
    "\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to the AWS SageMaker service console now, you should see that the endpoint creation is in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy using a batch transform job\n",
    "\n",
    "A batch transform job should be used for when you want to create inferences on a dateset and then shut down the resources when inference is finished.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config for batch transform\n",
    "batch_job_name='test-'+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())  #^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "output_location=f's3://{bucket}/{prefix}/test/pred.csv' #S3 bucket/location\n",
    "input_location= f's3://{bucket}/{prefix}/test/test.csv'  #S3 bucket/location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"TransformJobName\": batch_job_name,\n",
    "    \"ModelName\": model_name,\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": output_location,\n",
    "        \"Accept\": \"text/csv\",\n",
    "        \"AssembleWith\": \"Line\"\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": input_location \n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"SplitType\": \"Line\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    \"TransformResources\": {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\", #change this based on what resources you want to request\n",
    "            \"InstanceCount\": 1\n",
    "    }\n",
    "}\n",
    "sm.create_transform_job(**request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
